---
layout: about
title: About
permalink: /
subtitle: Graduate Student interested in Machine Learning and Natural Language Processing

profile:
  align: right
  image: yuzhegu_headshot.jpg
  image_circular: true # crops the image to make it circular
  more_info:

news: false # includes a list of news items
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---

<!-- ## About Me -->

I am a first year master student in [Department of Electrical and Systems Engineering](https://www.ese.upenn.edu) at the [University of Pennsylvania](https://www.upenn.edu) with a concentration on Machine Learning and Data Science. I received my dual B.Sc. in Data Science from [Duke Kunshan University](https://www.dukekunshan.edu.cn) and [Duke University](https://www.duke.edu), where I worked closely with [Peng Sun](https://scholars.duke.edu/person/Peng.Sun1) and [Enmao Diao](https://diaoenmao.com/). 

<!-- My research interest primarily revolves around enhancing **efficiency** and promoting **trustworthiness** in existing generative models. I am also interested in exploring the theory and application of learning quantized representations. My long-term objective is to develop intelligent systems that are both effective and reliable.  -->

My current research interest covers a wide range of topics related to deep generative models: 
- **Quantized Representation Learning**: quantized variational autoencoders, neural data compression, multimodality learning
- **Efficient LLMs**: quantization techniques for LLM speed up inference
- **Trustworthy LLMs**: bias, fairness, robustness

My overall objective is to develop intelligent systems that are both efficient and reliable.

<span style="color: blue;">I am currently seeking for a CS/ECE Ph.D. position related to Machine Learning and Natural Language Processing starting 2025 Fall. </span>

<table>
  <tr>
    <td>
      <strong>Email</strong>: tracygu (at) seas (dot) upenn (dot) edu<br>
      <strong>Google Scholar</strong>: <a href="https://scholar.google.com/citations?user=xdAB6asAAAAJ&hl=en">@scholar</a><br>
    </td>
  </tr>
  <tr>
    <td>
      <strong>Github</strong>: <a href="https://github.com/yzGuu830">@github/yzGuu830</a> <br>
      <strong>Linkedin</strong>: <a href="https://www.linkedin.com/in/yuzheguu">@in/yuzheguu</a><br>
    </td>
  </tr>
</table>
<br><br><br>

## Selected Publications 

<table>
  <tr>
    <td>
      <img src="../assets/img/publication_preview/esc.png" alt="Image description" width="200" style="margin-right: 80px;">
    </td>
    <td style="font-size: 15px;">
      <strong>ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers</strong><br>
      <strong>Yuzhe Gu</strong>, Enmao Diao<br>
      <em>arXiv, 2024</em><br>
      <a href="https://arxiv.org/abs/2404.19441">paper</a> / <a href="https://github.com/yzGuu830/efficient-speech-codec">code</a> 
      <br>
      <div style="font-size: 13px;">
      We propose Efficient Speech Codec (ESC), a lightweight parameter-efficient codec laid on cross-scale residual vector quantization and transformers. Our model leverages mirrored hierarchical window-attention transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. 
      </div>
    </td>
  </tr>
</table>

<br><br>

<table>
  <tr>
    <td>
      <img src="../assets/img/publication_preview/scd.png" alt="Image description" width="200" style="margin-right: 80px;">
    </td>
    <td style="font-size: 15px;">
      <strong>How Did We Get Here? Summarizing Conversation Dynamics</strong><br>
      Yilun Hua, Nicholas Chernogor, <strong>Yuzhe Gu</strong>, Seoyeon Julie Jeong, Miranda Luo, Cristian Danescu-Niculescu-Mizil<br>
      <em>Proceedings of NAACL, 2024</em><br>
      <a href="https://arxiv.org/abs/2404.19007">paper</a> / <a href="https://github.com/CornellNLP/scd?tab=readme-ov-file">code</a> 
      <br>
      <div style="font-size: 13px;">
      We introduce the task of summarizing the dynamics of conversations, by constructing a dataset of human-written summaries, and exploring several automated baselines. We evaluate whether such summaries can capture the trajectory of conversations via an established downstream task: forecasting whether an ongoing conversation will eventually derail into toxic behavior. We show that they help both humans and automated systems with this forecasting task.
      </div>
    </td>
  </tr>
</table>

<br><br>

<table>
  <tr>
    <td>
      <img src="../assets/img/publication_preview/policy.png" alt="Image description" width="200" style="margin-right: 80px;">
    </td>
    <td style="font-size: 15px;">
      <strong>Towards Quantification of Covid-19 Intervention Policies from Machine Learning-based Time Series Forecasting Approaches</strong><br>
      <strong>Yuzhe Gu</strong>, Peng Sun, Azzedine Boukerche<br>
      <em>Proceedings of IEEE International Conference on Communications (ICC), 2024</em><br>
      <a href="https://drive.google.com/file/d/1rFzjVxc8J8d316yVMEADwHNk9CAvbI0U/view">paper</a> / <a href="https://github.com/yzGuu830/epic-quant">code</a> 
      <br>
      <div style="font-size: 13px;">
      We design a policy-aware time series forecasting model to estimate COVID-19 trends by incorporating temporal information from 16 policy indicators. Through counterfactual analysis, we quantify the causal effect of indicators and propose two static metrics <em>lag period</em> and <em>average effect</em>. Our model verifies the effectiveness of all 16 policy indicators in controlling virus transmission in the US.
      </div>
    </td>
  </tr>
</table>